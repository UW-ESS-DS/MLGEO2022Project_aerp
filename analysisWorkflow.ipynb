{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fab560f-cf9f-43f4-9a0b-d335a6c75fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hdbscan in /srv/conda/envs/notebook/lib/python3.8/site-packages (0.8.29)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from hdbscan) (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from hdbscan) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from hdbscan) (1.9.3)\n",
      "Requirement already satisfied: cython>=0.27 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from hdbscan) (0.29.32)\n",
      "Requirement already satisfied: numpy>=1.20 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from hdbscan) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from scikit-learn>=0.20->hdbscan) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "#Importing modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, date, time\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "from pyproj import Geod\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statistics\n",
    "from scipy.stats import zscore\n",
    "!pip install hdbscan\n",
    "import hdbscan\n",
    "from functools import reduce\n",
    "import operator\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35978ee3-0c63-402e-9127-72812f460fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n"
     ]
    }
   ],
   "source": [
    "#Import data from files\n",
    "files=os.listdir('stations')\n",
    "stations=[]\n",
    "for fileName in files:\n",
    "    #Load data    \n",
    "    try:\n",
    "        file=np.loadtxt('stations/'+fileName, delimiter=\",\")\n",
    "        df = pd.DataFrame(file, columns = ['Time', 'xPos', 'yPos', 'xUncertainty', 'yUncertainty'])\n",
    "        #Ignore stations with high uncertainty\n",
    "        if df['xUncertainty'].mean()>3 or df['yUncertainty'].mean()>3:\n",
    "            continue\n",
    "        \n",
    "        #Drop unnecessary columns\n",
    "        df2=df.drop(columns=['xUncertainty', 'yUncertainty'])\n",
    "        \n",
    "        #Add additional columns to station data\n",
    "        df2['StationName']=fileName[:-4]\n",
    "        df2['Lat']=\"\"\n",
    "        df2['Lon']=\"\"\n",
    "        stations.append(df2)\n",
    "    except:\n",
    "        continue\n",
    "#Load key that describes station locations and station names\n",
    "lonLat=loadmat('positionsLonLat.mat')\n",
    "lonLat=lonLat['lonLat']\n",
    "lonLatKey=loadmat('stationNames.mat')\n",
    "lonLatKey=lonLatKey['names']\n",
    "print(len(stations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3570ea6-098f-467e-8fdf-7d70473b7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "2560\n",
      "2570\n",
      "2580\n",
      "2590\n",
      "2600\n",
      "2610\n",
      "2620\n",
      "2630\n",
      "2640\n",
      "2650\n",
      "2660\n",
      "2670\n",
      "2680\n",
      "2690\n",
      "2700\n",
      "2710\n",
      "2720\n",
      "2730\n",
      "2740\n",
      "2750\n",
      "2760\n",
      "2770\n",
      "2780\n",
      "2790\n",
      "2800\n",
      "2810\n",
      "2820\n",
      "2830\n",
      "2840\n",
      "2850\n",
      "2860\n",
      "2870\n",
      "2880\n",
      "2890\n",
      "2900\n",
      "2910\n",
      "2920\n",
      "2930\n",
      "2940\n",
      "2950\n",
      "2960\n",
      "2970\n",
      "2980\n",
      "2990\n",
      "3000\n",
      "3010\n",
      "3020\n",
      "3030\n",
      "3040\n",
      "3050\n",
      "3060\n",
      "3070\n",
      "3080\n",
      "3090\n",
      "3100\n",
      "3110\n",
      "3120\n",
      "3130\n",
      "3140\n",
      "3150\n",
      "3160\n",
      "3170\n",
      "3180\n",
      "3190\n",
      "3200\n",
      "3210\n",
      "3220\n",
      "3230\n",
      "3240\n",
      "3250\n",
      "3260\n",
      "3270\n",
      "3280\n",
      "3290\n",
      "3300\n",
      "3310\n",
      "3320\n",
      "3330\n",
      "3340\n",
      "3350\n",
      "3360\n",
      "3370\n",
      "3380\n",
      "3390\n",
      "3400\n",
      "3410\n",
      "3420\n",
      "3430\n",
      "3440\n",
      "3450\n",
      "3460\n",
      "3470\n",
      "3480\n",
      "3490\n",
      "3500\n",
      "3510\n",
      "3520\n",
      "3530\n",
      "3540\n",
      "3550\n",
      "3560\n",
      "3570\n",
      "3580\n",
      "3590\n",
      "3600\n",
      "3610\n",
      "3620\n",
      "3630\n",
      "3640\n"
     ]
    }
   ],
   "source": [
    "#Convert to data organized by timestep instead of by station\n",
    "\n",
    "g = Geod(ellps='clrk66') #used for calculating azimuth/magnitude\n",
    "\n",
    "#Collect desired time period\n",
    "perfectTimeSeries=np.arange(1995.99958 +(0.00274*5110), 1995.99958 +(0.00274*8760),0.00274) #2010-2020\n",
    "testPoint=2010.63 #start time\n",
    "difference_array = np.absolute(perfectTimeSeries-testPoint)\n",
    "index = difference_array.argmin()\n",
    "perfectTimeSeries=perfectTimeSeries#[index:index+300] #how far to go forward (in days)\n",
    "\n",
    "\n",
    "timesTotal=[]\n",
    "for value in range(0,len(perfectTimeSeries)):\n",
    "    timeOfficial=round(perfectTimeSeries[value], 5) #this is the timestep we're using\n",
    "    times = pd.DataFrame()\n",
    "    for value2 in range(0, len(stations)):\n",
    "        #Find which stations have data at that timestep\n",
    "        f = stations[value2]['Time']==timeOfficial\n",
    "        indices = f[f].index\n",
    "        if (len(list(indices)))==0:\n",
    "            continue\n",
    "        time=stations[value2].iloc[indices]\n",
    "        \n",
    "        #Add on location of that station\n",
    "        station=time.iloc[0]['StationName']\n",
    "        stationIndex=np.where(lonLatKey==station)[0][0]\n",
    "        time['Lat']=lonLat[stationIndex, 1]\n",
    "        time['Lon']=lonLat[stationIndex, 0]\n",
    "        \n",
    "        #Calculate magnitude and azimuth at that station\n",
    "        dLat = (time['yPos']/6378137) * 180/math.pi\n",
    "        dLon = (time['xPos']/(6378137*math.cos(math.pi*time['Lat']/180)))* 180/math.pi\n",
    "        az12,az21,dist = g.inv(time['Lon'],time['Lat'],time['Lon']+dLon,time['Lat']+dLat)\n",
    "        time['Mag']=dist\n",
    "        time['Azimuth']=az12\n",
    "        \n",
    "        #Drop unnecessary columns and reorganize\n",
    "        time=time.drop(columns=['StationName'])\n",
    "        time=time[['Time', 'xPos', 'yPos', 'Mag', 'Azimuth', 'Lat', 'Lon']]\n",
    "        times=pd.concat([times, time])\n",
    "    timesTotal.append(times)\n",
    "    if value % 10 == 0: #this just keeps track that we're making progress through the data\n",
    "        print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8facfc46-4ffe-405f-8d6e-e1b4d60faa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run clustering algorithm \n",
    "\n",
    "#Prep needed inputs\n",
    "def cluster(cluster_size, metric, samples, showFig):\n",
    "    timesTotal=timesTotal[0:100]\n",
    "    timesOneMatrix = pd.concat(timesTotal)\n",
    "    timesNP=timesOneMatrix.to_numpy()\n",
    "    labelsList=list()\n",
    "    dataPerWeek=list()\n",
    "    labelsPerWeek=list()\n",
    "    dataList=list()\n",
    "    indicesList=list()\n",
    "    for i in range(1,len(np.unique(timesNP[:,0]))):\n",
    "        #Prep data for clustering\n",
    "        data=timesTotal[i].to_numpy()\n",
    "        if len(data)<cluster_size*samples: # Make sure there is a significant amount of data\n",
    "            continue\n",
    "        #Calculate outliers\n",
    "        mask = np.absolute(zscore([x[1] for x in data])) > 1\n",
    "        mask2 = np.absolute(zscore([x[2] for x in data])) > 1\n",
    "        mask = np.logical_or(mask, mask2)\n",
    "        k=0\n",
    "        for sub_list in list(zip(*data)):\n",
    "            indices=[i for i, x in enumerate(mask) if ~x]\n",
    "            indicesList.append(indices)\n",
    "            dataList.append(data)\n",
    "        if i% 7==0:\n",
    "            out = list(reduce(operator.iand, map(set, indicesList))) #only including elements that aren't ever an outlier during the week\n",
    "            dataForPlotting=list()\n",
    "            for j in np.arange(0,7):\n",
    "                dataDF=pd.DataFrame()\n",
    "                for sub_list in list(zip(*dataList[j])):\n",
    "                    sub_list=[sub_list[i] for i in out]\n",
    "                    dataDF[str(k)]=sub_list\n",
    "                    k+=1\n",
    "                data=dataDF.to_numpy()\n",
    "                if len(data)<cluster_size*samples: # Make sure there is a significant amount of data\n",
    "                    continue\n",
    "                dataForPlotting.append(data)\n",
    "                #Normalize the data\n",
    "                dataforNorm=pd.DataFrame()\n",
    "                k=0\n",
    "                for sub_list in list(zip(*data)): #Dropping Time, xPos, yPos\n",
    "                    if k>=3:\n",
    "                        dataforNorm[str(k-3)]=sub_list\n",
    "                    k+=1\n",
    "                norm = MinMaxScaler()\n",
    "                dataNorm=norm.fit_transform(dataforNorm)\n",
    "\n",
    "                #Run clustering\n",
    "                clusterer = hdbscan.HDBSCAN(min_cluster_size=cluster_size, gen_min_span_tree=True, metric=metric, min_samples=samples).fit(dataNorm)\n",
    "                labels=clusterer.labels_\n",
    "                labelsList.append(labels)\n",
    "            if len(dataForPlotting)==0:\n",
    "                continue\n",
    "            dataForPlottingAvg = np.mean(dataForPlotting, 0)\n",
    "\n",
    "            #Every week, average the labels made in the clustering\n",
    "            #Ignore clusters with really small magnitude\n",
    "            nClusters=max(labels)+1\n",
    "            for m in range(0,nClusters):\n",
    "                k=0\n",
    "                dataDF=pd.DataFrame()\n",
    "                for sub_list in zip(*data):\n",
    "                    indices=[i for i, x in enumerate(labels) if x==m]\n",
    "                    sub_list=[sub_list[i] for i in indices]\n",
    "                    k+=1\n",
    "                    dataDF[str(k)]=sub_list\n",
    "                if abs((dataDF['2'].mean()+dataDF['3'].mean())/2)<0.2: \n",
    "                    labels[indices] = -1\n",
    "\n",
    "            #Calculate most common cluster and save data\n",
    "            labelsAvg = [statistics.mode(sub_list) for sub_list in zip(*labelsList)]\n",
    "            dataPerWeek.append(data)\n",
    "            labelsPerWeek.append(labelsAvg)\n",
    "\n",
    "            ##Show results for week\n",
    "            if showFig==\"True\":\n",
    "                figAverage, (ax1) = plt.subplots(1,1, figsize=(10,8)) \n",
    "                ax1.quiver([x[-1] for x in dataForPlottingAvg], [x[-2] for x in dataForPlottingAvg], [x[1] for x in dataForPlottingAvg], [x[2] for x in dataForPlottingAvg])\n",
    "                sc=ax1.scatter([x[-1] for x in dataForPlottingAvg], [x[-2] for x in dataForPlottingAvg], c=labelsAvg, edgecolor=\"k\")\n",
    "\n",
    "                ##Create legend\n",
    "                lp = lambda i: plt.plot([],color=sc.cmap(sc.norm(i)), mec=\"none\", label=\"Cluster {:g}\".format(i), ls=\"\", marker=\"o\")[0]\n",
    "                handles = [lp(i) for i in np.unique(labels)]\n",
    "                plt.legend(handles=handles)\n",
    "            labelsList=list()\n",
    "            dataList=list()\n",
    "            indicesList=list()\n",
    "    return [dataPerWeek, labelsPerWeek]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28a7487d-68a3-470c-8f10-471df1595adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/17\n",
      "0\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'timesTotal' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(counter)\n\u001b[1;32m     10\u001b[0m counter\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m [dataPerWeek, labelsPerWeek]\u001b[38;5;241m=\u001b[39m\u001b[43mcluster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFalse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m overallSilhouette\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     13\u001b[0m meanSum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[0;32mIn [9], line 5\u001b[0m, in \u001b[0;36mcluster\u001b[0;34m(cluster_size, metric, samples, showFig)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcluster\u001b[39m(cluster_size, metric, samples, showFig):\n\u001b[0;32m----> 5\u001b[0m     timesTotal\u001b[38;5;241m=\u001b[39m\u001b[43mtimesTotal\u001b[49m[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m100\u001b[39m]\n\u001b[1;32m      6\u001b[0m     timesOneMatrix \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(timesTotal)\n\u001b[1;32m      7\u001b[0m     timesNP\u001b[38;5;241m=\u001b[39mtimesOneMatrix\u001b[38;5;241m.\u001b[39mto_numpy()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'timesTotal' referenced before assignment"
     ]
    }
   ],
   "source": [
    "overallSilhouettes=[]\n",
    "hyperparameters=[]\n",
    "ratiosFullList=[]\n",
    "counter=0\n",
    "print(\"0/\"+str((3*len(np.arange(10,70,10))-1)))\n",
    "for cluster_size in np.arange(10,70,10):\n",
    "    for metric in ['euclidean','manhattan', 'chebyshev']:\n",
    "        samples=1\n",
    "        print(counter)\n",
    "        counter+=1\n",
    "        [dataPerWeek, labelsPerWeek]=cluster(cluster_size, metric, samples, \"False\")\n",
    "        overallSilhouette=0\n",
    "        meanSum=0\n",
    "        for i in np.arange(0,len(dataPerWeek)):\n",
    "            if len(np.unique(labelsPerWeek[i]))==0 or len(np.unique(labelsPerWeek[i]))==1:\n",
    "                continue\n",
    "            silhouette_avg = silhouette_score(dataPerWeek[i], labelsPerWeek[i])\n",
    "            overallSilhouette=overallSilhouette+silhouette_avg\n",
    "            meanSum+=1\n",
    "        if meanSum==0:\n",
    "            continue\n",
    "        overallSilhouette=overallSilhouette/meanSum\n",
    "        overallSilhouettes.append(overallSilhouette)\n",
    "\n",
    "        #consistent time clustering\n",
    "        ratios=0\n",
    "        trueLength=len(labelsPerWeek)\n",
    "        for count in np.arange(1,len(labelsPerWeek)):                   \n",
    "            if len(np.unique(labelsPerWeek[count]))==1: #nothing is clustered\n",
    "                ratio=0\n",
    "                trueLength=trueLength-1\n",
    "                continue\n",
    "            sm=difflib.SequenceMatcher(None,labelsPerWeek[count],labelsPerWeek[count-1])\n",
    "            ratio=sm.ratio()\n",
    "            ratios=ratios+ratio\n",
    "        ratios=ratios/trueLength\n",
    "        ratiosFullList.append(ratios)\n",
    "        hyperparameters.append([cluster_size, metric, samples])\n",
    "hyperparameters=hyperparameters[1:]\n",
    "overallSilhouettes=overallSilhouettes[1:]\n",
    "ratiosFullList=ratiosFullList[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8726c40-b4c7-4268-b9dd-aa454836affb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation minimum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mNormalizeData\u001b[39m(data):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (data \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(data)) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mmax(data) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(data))\n\u001b[0;32m---> 15\u001b[0m maxCombo\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39margpartition([a\u001b[38;5;241m*\u001b[39mb \u001b[38;5;28;01mfor\u001b[39;00m a,b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mNormalizeData\u001b[49m\u001b[43m(\u001b[49m\u001b[43moverallSilhouettes\u001b[49m\u001b[43m)\u001b[49m,NormalizeData(ratiosFullList))],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m:]\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(maxCombo)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m maxCombo:\n",
      "Cell \u001b[0;32mIn [11], line 13\u001b[0m, in \u001b[0;36mNormalizeData\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mNormalizeData\u001b[39m(data):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (data \u001b[38;5;241m-\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mmax(data) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(data))\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mamin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2918\u001b[0m, in \u001b[0;36mamin\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_amin_dispatcher)\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mamin\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, initial\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue,\n\u001b[1;32m   2804\u001b[0m          where\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue):\n\u001b[1;32m   2805\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2806\u001b[0m \u001b[38;5;124;03m    Return the minimum of an array or minimum along an axis.\u001b[39;00m\n\u001b[1;32m   2807\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2916\u001b[0m \u001b[38;5;124;03m    6\u001b[39;00m\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2919\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.8/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation minimum which has no identity"
     ]
    }
   ],
   "source": [
    "maxIndex=np.argpartition(overallSilhouettes, -4)[-4:]\n",
    "print(maxIndex)\n",
    "for item in maxIndex:\n",
    "    print(hyperparameters[item])\n",
    "    print(overallSilhouettes[item])\n",
    "    \n",
    "maxIndexRatios=np.argpartition(ratiosFullList, -4)[-4:]\n",
    "print(maxIndexRatios)\n",
    "for item in maxIndexRatios:\n",
    "    print(hyperparameters[item])\n",
    "    print(ratiosFullList[item])\n",
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "maxCombo=np.argpartition([a*b for a,b in zip(NormalizeData(overallSilhouettes),NormalizeData(ratiosFullList))],-4)[-4:]\n",
    "print(maxCombo)\n",
    "for item in maxCombo:\n",
    "    print(hyperparameters[item])\n",
    "    print(ratiosFullList[item])\n",
    "    print(overallSilhouettes[item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab957ff0-00a8-47f5-97a1-79d285b52f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[dataPerWeek, labelsPerWeek]=cluster(10, 'manhattan', 1, \"True\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
